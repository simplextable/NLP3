{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eksik sonuc verdi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yapılan işlemler onucu skor yükselmedi. Lokasyon, ve keyword gibi değişkenler concatlanıp bakılacak.\n",
    "Skoru yükselten asıl kaynağın model kurulması ve çalıştırılması olduğu anlaşıldı...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean(tweet): \n",
    "   \n",
    "    # Urls\n",
    "    tweet = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", tweet)\n",
    "        \n",
    "    # Words with punctuations and special characters\n",
    "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\"\n",
    "    for p in punctuations:\n",
    "        tweet = tweet.replace(p, f' {p} ')\n",
    "        \n",
    "    # ... and ..\n",
    "    tweet = tweet.replace('...', ' ... ')\n",
    "    if '...' not in tweet:\n",
    "        tweet = tweet.replace('..', ' ... ')      \n",
    "        \n",
    "    # Acronyms\n",
    "    tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", tweet)\n",
    "    tweet = re.sub(r\"mÌ¼sica\", \"music\", tweet)\n",
    "    tweet = re.sub(r\"okwx\", \"Oklahoma City Weather\", tweet)\n",
    "    tweet = re.sub(r\"arwx\", \"Arkansas Weather\", tweet)    \n",
    "    tweet = re.sub(r\"gawx\", \"Georgia Weather\", tweet)  \n",
    "    tweet = re.sub(r\"scwx\", \"South Carolina Weather\", tweet)  \n",
    "    tweet = re.sub(r\"cawx\", \"California Weather\", tweet)\n",
    "    tweet = re.sub(r\"tnwx\", \"Tennessee Weather\", tweet)\n",
    "    tweet = re.sub(r\"azwx\", \"Arizona Weather\", tweet)  \n",
    "    tweet = re.sub(r\"alwx\", \"Alabama Weather\", tweet)\n",
    "    tweet = re.sub(r\"wordpressdotcom\", \"wordpress\", tweet)    \n",
    "    tweet = re.sub(r\"usNWSgov\", \"United States National Weather Service\", tweet)\n",
    "    tweet = re.sub(r\"Suruc\", \"Sanliurfa\", tweet)   \n",
    "    \n",
    "    # Grouping same words without embeddings\n",
    "    tweet = re.sub(r\"Bestnaijamade\", \"bestnaijamade\", tweet)\n",
    "    tweet = re.sub(r\"SOUDELOR\", \"Soudelor\", tweet)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "\n",
    "df_train['text_cleaned'] = df_train['text'].apply(lambda s : clean(s))\n",
    "df_test['text_cleaned'] = df_test['text'].apply(lambda s : clean(s))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit',\n",
       " 'Hellfire! We don\\x89Ûªt even want to think about it or mention it so let\\x89Ûªs not do anything that leads to it #islam!',\n",
       " \"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\",\n",
       " 'In #islam saving a person is equal in reward to saving all humans! Islam is the opposite of terrorism!',\n",
       " 'To fight bioterrorism sir.',\n",
       " 'Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE',\n",
       " '#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption',\n",
       " '#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect',\n",
       " 'He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam',\n",
       " 'RT NotExplained: The only known image of infamous hijacker D.B. Cooper. http://t.co/JlzK2HdeTG',\n",
       " 'Hellfire is surrounded by desires so be careful and don\\x89Ûªt let your desires control you! #Afterlife',\n",
       " 'CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97/Georgia Ave Silver Spring',\n",
       " \"Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh...\",\n",
       " 'wowo--=== 12000 Nigerian refugees repatriated from Cameroon',\n",
       " '.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4',\n",
       " 'Caution: breathing may be hazardous to your health.',\n",
       " 'I Pledge Allegiance To The P.O.P.E. And The Burning Buildings of Epic City. ??????',\n",
       " 'that horrible sinking feeling when you\\x89Ûªve been at home on your phone for a while and you realise its been on 3G this whole time']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mislabeled = df_train.groupby(['text']).nunique().sort_values(by='target', ascending=False)\n",
    "df_mislabeled = df_mislabeled[df_mislabeled['target'] > 1]['target']\n",
    "df_mislabeled.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['target_relabeled'] = df_train['target'].copy() \n",
    "\n",
    "df_train.loc[df_train['text'] == 'like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit', 'target_relabeled'] = 0\n",
    "df_train.loc[df_train['text'] == 'Hellfire is surrounded by desires so be careful and donÛªt let your desires control you! #Afterlife', 'target_relabeled'] = 0\n",
    "df_train.loc[df_train['text'] == 'To fight bioterrorism sir.', 'target_relabeled'] = 0\n",
    "df_train.loc[df_train['text'] == '.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4', 'target_relabeled'] = 1\n",
    "df_train.loc[df_train['text'] == 'CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97/Georgia Ave Silver Spring', 'target_relabeled'] = 1\n",
    "df_train.loc[df_train['text'] == '#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption', 'target_relabeled'] = 0\n",
    "df_train.loc[df_train['text'] == 'In #islam saving a person is equal in reward to saving all humans! Islam is the opposite of terrorism!', 'target_relabeled'] = 0\n",
    "df_train.loc[df_train['text'] == 'Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE', 'target_relabeled'] = 1\n",
    "df_train.loc[df_train['text'] == 'RT NotExplained: The only known image of infamous hijacker D.B. Cooper. http://t.co/JlzK2HdeTG', 'target_relabeled'] = 1\n",
    "df_train.loc[df_train['text'] == \"Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh...\", 'target_relabeled'] = 0\n",
    "df_train.loc[df_train['text'] == \"wowo--=== 12000 Nigerian refugees repatriated from Cameroon\", 'target_relabeled'] = 0\n",
    "df_train.loc[df_train['text'] == \"He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\", 'target_relabeled'] = 0\n",
    "df_train.loc[df_train['text'] == \"Hellfire! We donÛªt even want to think about it or mention it so letÛªs not do anything that leads to it #islam!\", 'target_relabeled'] = 0\n",
    "df_train.loc[df_train['text'] == \"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\", 'target_relabeled'] = 0\n",
    "df_train.loc[df_train['text'] == \"Caution: breathing may be hazardous to your health.\", 'target_relabeled'] = 1\n",
    "df_train.loc[df_train['text'] == \"I Pledge Allegiance To The P.O.P.E. And The Burning Buildings of Epic City. ??????\", 'target_relabeled'] = 0\n",
    "df_train.loc[df_train['text'] == \"#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect\", 'target_relabeled'] = 0\n",
    "df_train.loc[df_train['text'] == \"that horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time\", 'target_relabeled'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split( df_train['text_cleaned']\n",
    "                                                     , df_train[\"target_relabeled\"],\n",
    "                                                       random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer() \n",
    "vectorizer.fit(train_x) \n",
    "\n",
    "x_train_tf_idf_word = vectorizer.transform(train_x)\n",
    "x_test_tf_idf_word = vectorizer.transform(test_x)\n",
    "\n",
    "x_train_tf_idf_word_n= x_train_tf_idf_word.toarray()\n",
    "x_test_tf_idf_word_n = x_test_tf_idf_word.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = df_test[\"text_cleaned\"]\n",
    "\n",
    "vectorizer.fit(test1) \n",
    "\n",
    "test_1 = vectorizer.transform(test1)\n",
    "\n",
    "test_1_n= test_1.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = np.zeros((3263,14244))\n",
    "result_test[:test_1_n.shape[0],:test_1_n.shape[1]] = test_1_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 7000)              99715000  \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 7000)              0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 100)               700100    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1000)              101000    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 100)               100100    \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 100,616,301\n",
      "Trainable params: 100,616,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Input - Layer\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(7000, activation = \"relu\", input_shape=(14244, )))\n",
    "# Hidden - Layers\n",
    "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(100, activation = \"relu\"))\n",
    "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(1000, activation = \"relu\"))\n",
    "model.add(layers.Dropout(0.1, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(100, activation = \"relu\"))\n",
    "# Output- Layer\n",
    "model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    " optimizer = \"adam\",\n",
    " loss = \"binary_crossentropy\",\n",
    " metrics = [\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5709 samples, validate on 1904 samples\n",
      "Epoch 1/3\n",
      "5709/5709 [==============================] - 47s 8ms/step - loss: 0.6545 - accuracy: 0.5884 - val_loss: 0.5403 - val_accuracy: 0.7820\n",
      "Epoch 2/3\n",
      "5709/5709 [==============================] - 47s 8ms/step - loss: 0.3549 - accuracy: 0.8821 - val_loss: 0.5935 - val_accuracy: 0.7973\n",
      "Epoch 3/3\n",
      "5709/5709 [==============================] - 45s 8ms/step - loss: 0.1003 - accuracy: 0.9671 - val_loss: 0.7206 - val_accuracy: 0.7679\n"
     ]
    }
   ],
   "source": [
    "results = model.fit(\n",
    " x_train_tf_idf_word_n, train_y,\n",
    " epochs= 3,\n",
    " batch_size = 500,\n",
    " validation_data = (x_test_tf_idf_word_n, test_y)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(result_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "y_pred1 = pd.DataFrame(data = y_pred, index = range(3263), columns=['target'])\n",
    "y_pred1[\"target\"] = y_pred1[\"target\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv') \n",
    "submission= submission[[\"id\"]]\n",
    "final_submission=pd.concat([submission,y_pred1],axis=1)\n",
    "final_submission.to_csv('10062020_3.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       0\n",
       "1   2       0\n",
       "2   3       0\n",
       "3   9       1\n",
       "4  11       0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
